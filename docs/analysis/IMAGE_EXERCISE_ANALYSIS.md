# Image-Based Exercise System Analysis

**Analysis Date:** October 2, 2025
**Project:** AVES - Interactive Spanish Bird Learning Platform
**Analyst:** Image-Based Exercise System Analyst

---

## Executive Summary

This analysis examines all image-based exercise workflows in the AVES platform, identifying three primary exercise types that rely on visual content: Visual Identification, Visual Discrimination, and Image Labeling. The analysis reveals a partially implemented system with significant gaps in image integration, data validation, and user experience optimization.

**Key Findings:**
- 2 of 3 image-based exercises are fully implemented (Visual Identification, Visual Discrimination)
- 1 exercise type defined but not implemented (Image Labeling)
- Visual Discrimination uses placeholder emoji instead of actual images
- Image data pipeline exists but lacks integration with exercise generation
- No image preloading or caching mechanisms
- Missing answer validation for coordinate-based interactions
- Incomplete metadata requirements for image quality/selection

---

## 1. Exercise Type Inventory

### 1.1 Visual Identification Exercise

**Status:** ‚úÖ IMPLEMENTED

**Purpose:** Users click on specific bird anatomy parts on an image to identify Spanish vocabulary terms.

**Component:** `frontend/src/components/exercises/VisualIdentification.tsx`

**Features:**
- Interactive hotspots over bird images
- Hardcoded anatomy maps for flamingo, eagle, sparrow
- Hover effects showing labels
- Visual feedback (green=correct, red=incorrect)
- Learning tips displayed after correct answers
- Uses external Unsplash images

**User Flow:**
```
1. User sees bird image with instruction: "Click on: [Spanish term]"
2. User hovers over hotspot regions ‚Üí labels appear
3. User clicks on region ‚Üí selection is recorded
4. System validates selection against targetPart
5. Visual feedback shown (border color changes)
6. Learning tip displayed if correct
```

**Data Requirements:**
- `exercise.metadata.bird` - Bird species identifier (flamingo/eagle/sparrow)
- `exercise.metadata.targetPart` - Anatomy part ID (beak/wings/tail/etc)
- `exercise.metadata.pronunciation` - Spanish pronunciation guide
- `exercise.metadata.tip` - Learning tip text
- `exercise.prompt` - Spanish term to identify

**Technical Implementation:**
```typescript
interface VisualIdentificationMetadata {
  bird: string;           // Species identifier
  targetPart: string;     // Correct anatomy part ID
  pronunciation?: string; // Pronunciation guide
  tip?: string;          // Learning tip
}

// Anatomy map structure (hardcoded)
type AnatomyMap = Array<{
  id: string;      // Part identifier
  label: string;   // Spanish term
  x: number;       // Percentage position X
  y: number;       // Percentage position Y
  width: number;   // Percentage width
  height: number;  // Percentage height
}>
```

**Images Used:**
- Hardcoded Unsplash URLs mapped to bird species
- No dynamic image selection
- No validation of image availability
- No local caching

**Issues Identified:**
1. Hardcoded anatomy coordinates only work with specific images
2. Only 3 bird species supported (flamingo, eagle, sparrow)
3. No dynamic anatomy detection from annotations
4. Images load synchronously (potential performance issue)
5. No fallback if external images fail to load
6. Coordinates are percentages, may not align with all image aspect ratios

---

### 1.2 Visual Discrimination Exercise

**Status:** ‚ö†Ô∏è PARTIALLY IMPLEMENTED (Missing Image Integration)

**Purpose:** Users select the correct image from multiple options that matches a given Spanish term.

**Component:** `frontend/src/components/exercises/VisualDiscrimination.tsx`

**Current Implementation:**
- Grid of 4 option buttons
- Placeholder emoji (ü¶Ö) instead of actual images
- Letter labels (A, B, C, D)
- Visual feedback on selection
- Disabled state after answer submission

**Expected Features (Not Implemented):**
- Display actual bird images from annotation data
- Load images from `option.imageUrl` field
- Image lazy loading
- Image error handling
- Responsive image sizing

**User Flow:**
```
1. User sees Spanish term: "[target term]"
2. User views 4 image options (currently emoji placeholders)
3. User clicks on an image
4. System validates against correctOptionId
5. Correct answer highlighted in green, incorrect in red
6. Other options dimmed
```

**Data Requirements:**
- `exercise.targetTerm` - Spanish term to match
- `exercise.options` - Array of option objects:
  - `id` - Unique option identifier
  - `imageUrl` - Path to image (DEFINED BUT NOT USED)
  - `species` - Species name
- `exercise.correctOptionId` - ID of correct option

**Technical Implementation:**
```typescript
interface VisualDiscriminationExercise {
  type: 'visual_discrimination';
  targetTerm: string;
  options: Array<{
    id: string;
    imageUrl: string;  // ‚ùå DEFINED BUT NOT RENDERED
    species: string;
  }>;
  correctOptionId: string;
}
```

**Critical Gap:**
```tsx
// Current implementation (line 67-70)
<div className="aspect-square bg-gray-200 rounded-lg mb-2 flex items-center justify-center">
  {/* In production, this would be an actual image */}
  <span className="text-4xl">ü¶Ö</span>
</div>

// Should be:
<div className="aspect-square bg-gray-200 rounded-lg mb-2 overflow-hidden">
  <img
    src={option.imageUrl}
    alt={option.species}
    className="w-full h-full object-cover"
    onError={handleImageError}
  />
</div>
```

**Issues Identified:**
1. **CRITICAL:** Images not rendered - only emoji placeholders
2. No image preloading mechanism
3. No error handling for failed image loads
4. No loading states for images
5. No image optimization (size, format)
6. No accessibility attributes (alt text is basic)

---

### 1.3 Image Labeling Exercise

**Status:** ‚ùå NOT IMPLEMENTED

**Purpose:** Users drag labels to correct positions on a bird image to identify anatomical parts.

**Type Definition:** `shared/types/exercise.types.ts`

**Defined Interface:**
```typescript
interface ImageLabelingExercise extends ExerciseBase {
  type: 'image_labeling';
  imageUrl: string;
  labels: Array<{
    id: string;
    term: string;
    correctPosition: { x: number; y: number };
  }>;
}
```

**Expected Features (Not Implemented):**
- Drag-and-drop interaction
- Image display with label placement zones
- Validation of label positions (proximity to correct coordinates)
- Visual connection lines from labels to image points
- Feedback on label accuracy

**Missing Components:**
- No React component (`ImageLabeling.tsx` does not exist)
- Not referenced in `ExerciseContainer.tsx` switch statement
- No generation logic in `EnhancedExerciseGenerator.ts`
- No answer checking logic

**Implementation Gap:**
```typescript
// Missing in ExerciseContainer.tsx (line 99-126)
case 'image_labeling':
  exerciseComponent = (
    <ImageLabeling
      exercise={currentExercise as any}
      onAnswer={handleAnswer}
      disabled={showFeedback}
    />
  );
  break;

// Missing in EnhancedExerciseGenerator.ts
private generateImageLabeling(): EnhancedExercise | null {
  // TODO: Implementation needed
}

// Missing in EnhancedExerciseGenerator.checkAnswer() (line 422-448)
case 'image_labeling':
  return this.checkLabelPositions(userAnswer, metadata.labels);
```

**Recommended Implementation Priority:** HIGH
- Provides hands-on interactive learning
- Complements visual identification exercises
- Utilizes existing annotation coordinate data

---

## 2. Exercise Generator Analysis

### 2.1 EnhancedExerciseGenerator Service

**File:** `frontend/src/services/enhancedExerciseGenerator.ts`

**Architecture:**
```typescript
class EnhancedExerciseGenerator {
  private annotations: Annotation[]
  private currentLevel: number
  private exerciseHistory: string[]

  // Adaptive difficulty selection
  generateAdaptiveExercise(): EnhancedExercise | null

  // Type-specific generators
  private generateVisualIdentification(): EnhancedExercise | null
  private generateEnhancedVisualDiscrimination(): EnhancedExercise | null
  private generateImageLabeling(): NOT IMPLEMENTED
}
```

### 2.2 Image Selection for Visual Identification

**Current Implementation (Lines 65-92):**

```typescript
private generateVisualIdentification(): EnhancedExercise | null {
  // 1. Filter by annotation type
  const anatomicalAnnotations = this.annotations.filter(a => a.type === 'anatomical');

  // 2. Random selection
  const target = anatomicalAnnotations[Math.floor(Math.random() * anatomicalAnnotations.length)];

  // 3. Get bird ID from annotation
  const bird = target.imageId || 'flamingo';

  // 4. Map term to anatomy part
  const targetPart = this.mapTermToPart(target.spanishTerm);

  // Returns metadata with bird ID and target part
}
```

**Issues:**
1. **Mismatch:** `annotation.imageId` contains bird species name, but component expects specific anatomy maps
2. **Fallback:** Defaults to 'flamingo' if imageId missing
3. **No Validation:** Doesn't check if anatomy map exists for the bird
4. **Limited Mapping:** Only 9 Spanish terms mapped to anatomy parts

**Term Mapping (Lines 318-331):**
```typescript
private mapTermToPart(spanishTerm: string): string {
  const mapping: Record<string, string> = {
    'el pico': 'beak',
    'las patas': 'legs',
    'las alas': 'wings',
    'las garras': 'talons',
    'los ojos': 'eyes',
    'el cuello': 'neck',
    'las plumas': 'feathers',
    'la cola': 'tail',
    'el pecho': 'breast'
  };
  return mapping[spanishTerm] || 'beak';
}
```

**Missing:**
- No validation that target part exists in selected bird's anatomy map
- No difficulty-based selection (all parts equal difficulty)
- No checking of bounding box data from annotations

### 2.3 Image Selection for Visual Discrimination

**Current Implementation (Lines 94-123):**

```typescript
private generateEnhancedVisualDiscrimination(): EnhancedExercise | null {
  // 1. Filter by current difficulty level
  const byDifficulty = this.annotations.filter(a => a.difficultyLevel === this.currentLevel);
  const pool = byDifficulty.length >= 4 ? byDifficulty : this.annotations;

  // 2. Shuffle and select 1 correct + 3 distractors
  const shuffled = [...pool].sort(() => Math.random() - 0.5);
  const correct = shuffled[0];
  const distractors = shuffled.slice(1, 4);

  // 3. Map to exercise options
  metadata: {
    correctId: correct.id,
    options: [correct, ...distractors].map(a => ({
      id: a.id,
      term: a.spanishTerm,
      image: a.imageId,    // ‚ùå NOT USED BY COMPONENT
      hint: a.englishTerm
    }))
  }
}
```

**Critical Issues:**
1. **Data Loss:** `a.imageId` mapped to `image` field, but component expects `imageUrl`
2. **No URL Generation:** Doesn't construct actual image URLs
3. **No Image Validation:** Doesn't check if images exist or are accessible
4. **No Distractor Strategy:** Random selection without considering visual similarity

**Expected vs Actual:**
```typescript
// What component expects:
options: Array<{
  id: string;
  imageUrl: string;  // Full URL to image
  species: string;
}>

// What generator provides:
options: Array<{
  id: string;
  term: string;      // Spanish term
  image: string;     // Image ID (not URL)
  hint: string;      // English term
}>
```

**Image URL Construction Missing:**
```typescript
// Should exist somewhere:
function getImageUrl(imageId: string): string {
  // Query image_sources table for local_path or original_url
  // Or construct URL from image ID
  return `/api/images/${imageId}` || `/uploads/images/${imageId}.jpg`;
}
```

---

## 3. Exercise Rendering Components

### 3.1 VisualIdentification Component

**File:** `frontend/src/components/exercises/VisualIdentification.tsx`

**Strengths:**
- Clean interactive UI with hover states
- Good visual feedback (color-coded borders)
- Responsive hotspot system
- Accessibility: Shows labels on hover/selection
- Performance: Uses React.memo callbacks and useMemo for maps

**Weaknesses:**
1. **Hardcoded Images:** Static Unsplash URLs (lines 106-116)
2. **Limited Species:** Only flamingo, eagle, sparrow, stork, owl, peacock
3. **Fixed Anatomy Maps:** Coordinates don't adapt to different images
4. **No Dynamic Hotspots:** Can't use annotation bounding box data
5. **No Image Loading State:** No spinner while image loads
6. **No Error Handling:** No fallback if image fails

**Image Hardcoding Example:**
```typescript
const birdImage = useMemo(() => {
  const images: Record<string, string> = {
    flamingo: 'https://images.unsplash.com/photo-1535821265819-8e7ff3c30737?w=600',
    eagle: 'https://images.unsplash.com/photo-1611689342806-0863700ce1e4?w=600',
    // ... more hardcoded URLs
  };
  return images[targetBird] || images.flamingo;
}, [targetBird]);
```

**Should Be:**
```typescript
const { data: birdImage, isLoading, error } = useImageQuery(targetBird);

if (isLoading) return <ImageLoadingSpinner />;
if (error) return <ImageErrorFallback />;
```

**Anatomy Map Limitations:**
- Coordinates are percentage-based (good for responsiveness)
- But only work with specific image compositions
- Different bird poses would break coordinate alignment
- No relationship to annotation bounding boxes

### 3.2 VisualDiscrimination Component

**File:** `frontend/src/components/exercises/VisualDiscrimination.tsx`

**Strengths:**
- Clean grid layout (2x2)
- Clear letter labels (A, B, C, D)
- Good state management (selected, answered)
- Visual feedback system works well

**Critical Weakness:**
**LINE 67-70: PLACEHOLDER EMOJI INSTEAD OF IMAGES**

```typescript
<div className="aspect-square bg-gray-200 rounded-lg mb-2 flex items-center justify-center">
  {/* In production, this would be an actual image */}
  <span className="text-4xl">ü¶Ö</span>
</div>
```

**Impact:**
- Exercise is non-functional for actual learning
- Users can't visually discriminate bird features
- Defeats the purpose of "visual" discrimination
- Comment acknowledges incompleteness

**Required Fix:**
```typescript
<div className="aspect-square bg-gray-200 rounded-lg mb-2 overflow-hidden relative">
  {imageLoading && <LoadingSpinner />}
  <img
    src={option.imageUrl}
    alt={`Bird option: ${option.species}`}
    className="w-full h-full object-cover"
    onLoad={() => setImageLoading(false)}
    onError={(e) => {
      e.currentTarget.src = '/assets/placeholder-bird.jpg';
      logImageError(option.id, option.imageUrl);
    }}
    loading="lazy"
  />
</div>
```

### 3.3 ExerciseContainer (Orchestrator)

**File:** `frontend/src/components/exercises/ExerciseContainer.tsx`

**Responsibilities:**
- Manages exercise lifecycle
- Renders appropriate exercise component
- Handles answer submission
- Displays progress and feedback

**Exercise Routing (Lines 99-141):**
```typescript
switch (currentExercise.type) {
  case 'visual_identification':
    return <VisualIdentification ... />;

  case 'visual_discrimination':
    return <VisualDiscrimination ... />;

  case 'contextual_fill':
    return <ContextualFill ... />;

  // ‚ùå MISSING: case 'image_labeling'

  default:
    return <FallbackExercise />;
}
```

**Issues:**
1. No image preloading before exercise display
2. No image prefetching for upcoming exercises
3. Type casting to `any` loses type safety (lines 103, 113, 122)
4. No error boundary for exercise rendering failures

**Improvement Needed:**
```typescript
// Preload next exercise images
useEffect(() => {
  const nextExercise = generator.peekNextExercise();
  if (nextExercise && isImageExercise(nextExercise)) {
    preloadExerciseImages(nextExercise);
  }
}, [currentExercise]);
```

---

## 4. Data Requirements Analysis

### 4.1 Annotation Data Structure

**Source:** `shared/types/annotation.types.ts`

```typescript
interface Annotation {
  id: string;
  imageId: string;              // ‚ö†Ô∏è Used inconsistently
  boundingBox: BoundingBox;     // ‚ùå NOT USED in exercises
  type: AnnotationType;         // Used for filtering
  spanishTerm: string;          // Used in exercises
  englishTerm: string;          // Used in hints
  pronunciation?: string;       // Used in Visual ID
  difficultyLevel: 1 | 2 | 3 | 4 | 5;  // Used in adaptive generation
  isVisible: boolean;
  createdAt: Date;
  updatedAt: Date;
}

interface BoundingBox {
  topLeft: Coordinate;
  bottomRight: Coordinate;
  width: number;
  height: number;
}
```

**Data Usage in Exercises:**

| Field | Visual ID | Visual Disc | Image Label | Notes |
|-------|-----------|-------------|-------------|-------|
| id | ‚ùå | ‚úÖ | ‚úÖ | For validation |
| imageId | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚úÖ | Used as species name, not image reference |
| boundingBox | ‚ùå | ‚ùå | ‚ùå | **NOT USED ANYWHERE** |
| type | ‚úÖ | ‚úÖ | ‚úÖ | Filtering anatomical annotations |
| spanishTerm | ‚úÖ | ‚úÖ | ‚úÖ | Primary term to teach |
| englishTerm | ‚úÖ | ‚úÖ | ‚úÖ | Translation/hint |
| pronunciation | ‚úÖ | ‚ö†Ô∏è | ‚ùå | Only in Visual ID |
| difficultyLevel | ‚úÖ | ‚úÖ | ‚ùå | Adaptive generation |
| isVisible | ‚ùå | ‚ùå | ‚ùå | Should filter but doesn't |

**Critical Gap: Bounding Box Data Unused**

The annotation system captures precise bounding box coordinates for each anatomical feature, but exercises don't use this data. This means:

1. Visual Identification could dynamically generate hotspots from bounding boxes
2. Image Labeling could validate label placement against bounding boxes
3. Difficulty could scale based on bounding box size (smaller = harder)

**Current:** Hardcoded coordinates in component
**Should Be:** Dynamic hotspots from annotation.boundingBox

### 4.2 Image Data Structure

**Source:** `shared/types/annotation.types.ts`, `shared/types/image.types.ts`

```typescript
// From annotation.types.ts
interface Image {
  id: string;
  url: string;                  // ‚úÖ DEFINED
  thumbnailUrl?: string;        // ‚úÖ DEFINED
  species: string;
  scientificName: string;
  source: 'unsplash' | 'midjourney' | 'uploaded';
  width: number;
  height: number;
  annotations: Annotation[];    // ‚ùå Relationship exists but not queried
  metadata?: {
    photographer?: string;
    license?: string;
    tags?: string[];
  };
}

// From image.types.ts
interface ImageSource {
  id: string;
  speciesId: string;
  sourceType: ImageSourceType;
  originalUrl: string;          // ‚úÖ AVAILABLE
  localPath?: string;           // ‚úÖ AVAILABLE
  thumbnailPath?: string;       // ‚úÖ AVAILABLE
  width: number;
  height: number;
  photographer?: { name, url };
  license: LicenseType;
}
```

**Missing Link:**
- Annotations have `imageId` field
- Images table exists in backend (`backend/src/routes/images.ts`)
- But no API endpoint to get image by ID
- Exercises can't resolve imageId ‚Üí actual image URL

**Required API Endpoint:**
```typescript
// GET /api/images/:imageId
{
  id: string,
  url: string,
  thumbnailUrl: string,
  species: string,
  annotations: Annotation[]
}
```

### 4.3 Image Quality Requirements

**Current State:** No defined requirements

**Recommended Requirements:**

| Requirement | Value | Rationale |
|-------------|-------|-----------|
| Minimum Width | 800px | Clarity for anatomy identification |
| Minimum Height | 600px | Aspect ratio for birds |
| Aspect Ratio | 4:3 or 3:2 | Standard bird photography |
| Format | JPEG/WebP | Browser compatibility + compression |
| Max File Size | 500KB | Performance (original), 150KB (thumbnail) |
| Orientation | Landscape preferred | Bird poses typically horizontal |
| Background | Blur/clean preferred | Focus on subject |
| Bird Visibility | >60% of frame | Anatomy features visible |
| Image Quality | No artifacts | Clear details for learning |

**Missing Validation:**
- No checks in image import process
- No quality scoring system
- No automatic rejection of poor images
- No user feedback on image suitability

---

## 5. Exercise Flow Analysis

### 5.1 Visual Identification Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ START: Visual Identification Exercise                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Exercise Generator                                            ‚îÇ
‚îÇ    - Filter anatomical annotations                              ‚îÇ
‚îÇ    - Random selection of target term                            ‚îÇ
‚îÇ    - Extract imageId (bird species)                             ‚îÇ
‚îÇ    - Map Spanish term ‚Üí anatomy part                            ‚îÇ
‚îÇ    - Create exercise with metadata                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Exercise Rendering                                            ‚îÇ
‚îÇ    - Extract bird species from metadata                         ‚îÇ
‚îÇ    - Lookup hardcoded anatomy map                               ‚îÇ
‚îÇ    - Lookup hardcoded Unsplash image URL                        ‚îÇ
‚îÇ    - Render image with overlay hotspots                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. User Interaction                                              ‚îÇ
‚îÇ    - User hovers over regions ‚Üí labels appear                   ‚îÇ
‚îÇ    - User clicks on region ‚Üí selection recorded                 ‚îÇ
‚îÇ    - handlePartClick(partId) called                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. Answer Validation                                             ‚îÇ
‚îÇ    - Compare partId with metadata.targetPart                    ‚îÇ
‚îÇ    - Set visual feedback (green/red border)                     ‚îÇ
‚îÇ    - Display learning tip if correct                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. Progress Tracking                                             ‚îÇ
‚îÇ    - Update session progress (correct/total)                    ‚îÇ
‚îÇ    - Update current streak                                      ‚îÇ
‚îÇ    - Send result to backend API                                 ‚îÇ
‚îÇ    - 3-second delay ‚Üí generate next exercise                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
                          [END]
```

**Pain Points:**
1. **Line: Generator ‚Üí Renderer** - Data mismatch (imageId vs actual image)
2. **Line: Rendering** - Hardcoded lookups could fail
3. **Line: Interaction** - No loading state, image could still be loading
4. **Line: Validation** - No coordinate-based validation (click accuracy)

### 5.2 Visual Discrimination Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ START: Visual Discrimination Exercise                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Exercise Generator                                            ‚îÇ
‚îÇ    - Filter by difficulty level                                 ‚îÇ
‚îÇ    - Select 1 correct + 3 distractors                           ‚îÇ
‚îÇ    - Create options with imageId                                ‚îÇ
‚îÇ    ‚ùå BUT: No imageUrl generated                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Exercise Rendering                                            ‚îÇ
‚îÇ    ‚ùå BROKEN: Renders emoji instead of images                   ‚îÇ
‚îÇ    - Grid of 4 options                                          ‚îÇ
‚îÇ    - Letter labels shown                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. User Interaction                                              ‚îÇ
‚îÇ    - User clicks on emoji placeholder                           ‚îÇ
‚îÇ    - handleSelect(optionId) called                              ‚îÇ
‚îÇ    - Selection disabled after first answer                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. Answer Validation                                             ‚îÇ
‚îÇ    - Compare optionId with correctOptionId                      ‚îÇ
‚îÇ    - Show green/red borders                                     ‚îÇ
‚îÇ    - Checkmark/X icons displayed                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. Progress & Next Exercise                                      ‚îÇ
‚îÇ    - Update progress stats                                      ‚îÇ
‚îÇ    - 3-second delay ‚Üí next exercise                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
                          [END]
```

**Critical Failure:**
- **Step 2** is completely broken - no images displayed
- Exercise cannot function as intended
- Users can't learn visual discrimination without images

### 5.3 Image Labeling Flow (Not Implemented)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ START: Image Labeling Exercise (PLANNED)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Exercise Generator (NOT IMPLEMENTED)                         ‚îÇ
‚îÇ    - Select bird image with multiple annotations               ‚îÇ
‚îÇ    - Create label list from annotations                         ‚îÇ
‚îÇ    - Set correct positions from bounding boxes                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Exercise Rendering (COMPONENT MISSING)                       ‚îÇ
‚îÇ    - Display bird image                                         ‚îÇ
‚îÇ    - Show draggable labels (Spanish terms)                      ‚îÇ
‚îÇ    - Provide drop zones or freeform placement                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. User Interaction (NOT IMPLEMENTED)                           ‚îÇ
‚îÇ    - User drags label from list                                 ‚îÇ
‚îÇ    - User drops label on image at chosen position               ‚îÇ
‚îÇ    - Repeat for all labels                                      ‚îÇ
‚îÇ    - Submit when all labels placed                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. Answer Validation (NOT IMPLEMENTED)                          ‚îÇ
‚îÇ    - Check each label position vs. correctPosition              ‚îÇ
‚îÇ    - Allow tolerance (e.g., within 50px)                        ‚îÇ
‚îÇ    - Calculate accuracy score                                   ‚îÇ
‚îÇ    - Visual feedback: connection lines to correct positions     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. Progress & Feedback (NOT IMPLEMENTED)                        ‚îÇ
‚îÇ    - Show which labels were correctly placed                    ‚îÇ
‚îÇ    - Highlight incorrect placements                             ‚îÇ
‚îÇ    - Provide correction guidance                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
                          [END]
```

---

## 6. Issues & Gaps Summary

### 6.1 Critical Issues (Blocking Basic Functionality)

| Issue | Exercise Type | Impact | Priority |
|-------|---------------|--------|----------|
| Visual Discrimination shows emoji not images | Visual Discrimination | Exercise non-functional | CRITICAL |
| Image Labeling not implemented | Image Labeling | Feature completely missing | HIGH |
| No image URL resolution | All | Can't load images from annotations | CRITICAL |
| Hardcoded images only | Visual ID | Limited to 6 bird species | HIGH |

### 6.2 Data Integration Issues

| Issue | Location | Impact |
|-------|----------|--------|
| imageId doesn't map to actual images | Exercise Generator | Data pipeline broken |
| Bounding box data unused | All exercises | Precision learning opportunity lost |
| No image preloading | ExerciseContainer | Poor UX, loading delays |
| isVisible filter not applied | Exercise generation | Could show hidden annotations |
| No image quality validation | Image import | Poor quality images in system |

### 6.3 Performance Issues

| Issue | Impact | Solution |
|-------|--------|----------|
| Synchronous image loading | UI blocks while images load | Implement lazy loading |
| No image caching | Same images reload repeatedly | Service worker caching |
| External Unsplash dependency | Slow loads, possible failures | Local image storage |
| No thumbnail usage | Large images load in grids | Use thumbnails for discrimination |
| No image prefetching | Delay between exercises | Preload next exercise images |

### 6.4 User Experience Issues

| Issue | Exercise | Impact |
|-------|----------|--------|
| No image loading spinner | All | User doesn't know image is loading |
| No error fallback | All | Broken image = broken exercise |
| No image attribution | All | Licensing compliance risk |
| Hardcoded anatomy maps fragile | Visual ID | Breaks with different images |
| No difficulty progression feedback | All | User doesn't know why exercises change |

### 6.5 Validation & Accuracy Issues

| Issue | Exercise | Impact |
|-------|----------|--------|
| No coordinate accuracy tolerance | Visual ID | Only exact clicks count |
| No partial credit | Image Labeling | Binary correct/incorrect |
| No validation of hotspot coverage | Visual ID | Gaps in clickable areas |
| No checking of answer plausibility | All | Random guessing not detected |

### 6.6 Missing Features

| Feature | Exercise | Value |
|---------|----------|-------|
| Zoom on image | All | Better detail inspection |
| Image comparison mode | Visual Discrimination | Side-by-side comparison |
| Hint system with visual cues | All | Guided learning |
| Replay/review mode | All | Learn from mistakes |
| Annotation editing | Admin | Improve data quality |
| Multi-language support | All | Beyond Spanish/English |

---

## 7. Recommendations

### 7.1 Immediate Fixes (Sprint 1)

**Priority 1: Make Visual Discrimination Functional**
```typescript
// File: VisualDiscrimination.tsx
// Replace lines 67-70 with:
<div className="aspect-square bg-gray-200 rounded-lg overflow-hidden">
  <img
    src={option.imageUrl}
    alt={option.species}
    className="w-full h-full object-cover"
    loading="lazy"
  />
</div>
```

**Priority 2: Fix Image URL Resolution**
```typescript
// File: enhancedExerciseGenerator.ts
// In generateEnhancedVisualDiscrimination(), add:
const imageUrlMap = await fetchImageUrls([correct.id, ...distractors.map(d => d.id)]);

options: [correct, ...distractors].map(a => ({
  id: a.id,
  imageUrl: imageUrlMap[a.imageId] || '/assets/placeholder.jpg',
  species: a.spanishTerm
}))
```

**Priority 3: Add Image Loading States**
```typescript
// Create shared component: ImageWithLoading.tsx
const ImageWithLoading = ({ src, alt, onError }) => {
  const [loading, setLoading] = useState(true);

  return (
    <div className="relative">
      {loading && <Spinner />}
      <img
        src={src}
        alt={alt}
        onLoad={() => setLoading(false)}
        onError={onError}
      />
    </div>
  );
};
```

### 7.2 Short-Term Improvements (Sprint 2-3)

**1. Implement Image Labeling Exercise**
- Create `ImageLabeling.tsx` component
- Implement drag-and-drop with react-dnd
- Use annotation bounding boxes for validation
- Add visual feedback (connection lines, distance indicators)

**2. Dynamic Hotspot Generation for Visual ID**
```typescript
// Use annotation.boundingBox instead of hardcoded maps
const generateHotspots = (annotations: Annotation[]) => {
  return annotations.map(ann => ({
    id: ann.id,
    label: ann.spanishTerm,
    x: ann.boundingBox.topLeft.x,
    y: ann.boundingBox.topLeft.y,
    width: ann.boundingBox.width,
    height: ann.boundingBox.height
  }));
};
```

**3. Image Preloading Service**
```typescript
// services/imagePreloader.ts
class ImagePreloader {
  private cache: Map<string, HTMLImageElement> = new Map();

  async preload(urls: string[]): Promise<void> {
    await Promise.all(urls.map(url => this.loadImage(url)));
  }

  private loadImage(url: string): Promise<HTMLImageElement> {
    if (this.cache.has(url)) return Promise.resolve(this.cache.get(url)!);

    return new Promise((resolve, reject) => {
      const img = new Image();
      img.onload = () => {
        this.cache.set(url, img);
        resolve(img);
      };
      img.onerror = reject;
      img.src = url;
    });
  }
}
```

**4. Image Quality Validation**
```typescript
// Add to backend image import:
async function validateImageQuality(imageUrl: string): Promise<boolean> {
  const metadata = await sharp(imageBuffer).metadata();

  return (
    metadata.width >= 800 &&
    metadata.height >= 600 &&
    metadata.format === 'jpeg' || metadata.format === 'webp'
  );
}
```

### 7.3 Long-Term Enhancements (Sprint 4+)

**1. AI-Powered Image Selection**
- Use computer vision to score image quality
- Detect bird visibility percentage
- Filter out occluded or poor-quality images
- Rank images by learning value

**2. Adaptive Difficulty Based on Image Complexity**
```typescript
interface ImageComplexity {
  birdSize: number;        // % of frame
  backgroundClutter: number;  // Complexity score
  anatomyVisibility: {     // Which parts visible
    [part: string]: boolean;
  };
  difficulty: 1 | 2 | 3 | 4 | 5;
}

// Select images matching user skill level
```

**3. Multi-Image Exercises**
- Compare/contrast exercises with 2+ images
- "Spot the difference" for anatomy learning
- "Which image shows X feature" with subtle variations

**4. Annotated Image Gallery**
- Browse mode with all annotations visible
- Study mode before exercises
- Reference mode during practice

**5. User-Generated Content**
- Allow users to upload bird photos
- Community annotation system
- Peer review for quality control

### 7.4 Technical Debt Resolution

**1. Refactor Exercise Generation**
```typescript
// Separate concerns:
class ExerciseGenerator {
  constructor(
    private dataSource: IExerciseDataSource,
    private difficultyEngine: IDifficultyEngine,
    private imageResolver: IImageResolver
  ) {}
}

// Make testable and extensible
```

**2. Type Safety Improvements**
```typescript
// Remove `as any` casts in ExerciseContainer
// Create proper type guards for each exercise type
function isVisualDiscriminationExercise(
  ex: EnhancedExercise
): ex is VisualDiscriminationExercise {
  return ex.type === 'visual_discrimination' && 'options' in ex;
}
```

**3. API Layer for Images**
```typescript
// Create dedicated image API service
class ImageAPI {
  async getImageById(id: string): Promise<Image>
  async getImagesBySpecies(species: string): Promise<Image[]>
  async getImagesForExercise(type: ExerciseType, count: number): Promise<Image[]>
}
```

---

## 8. Success Metrics

### 8.1 Functional Completeness

| Metric | Current | Target | How to Measure |
|--------|---------|--------|----------------|
| Image-based exercises implemented | 2/3 (67%) | 3/3 (100%) | Code coverage |
| Exercises using real images | 1/3 (33%) | 3/3 (100%) | Manual testing |
| Image loading success rate | Unknown | >99% | Error logging |
| Bounding box data utilization | 0% | 100% | Code analysis |

### 8.2 Performance Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| Image load time (median) | <500ms | Performance API |
| Exercise transition time | <200ms | Time between exercises |
| Cache hit rate | >80% | Service worker metrics |
| Failed image loads | <1% | Error tracking |

### 8.3 User Experience Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| Exercise completion rate | >85% | Session tracking |
| Average time per exercise | 10-15s | Analytics |
| User-reported image issues | <5% | Feedback form |
| Accuracy improvement over session | +15% | Progress tracking |

### 8.4 Data Quality Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| Images with annotations | >80% | Database query |
| Average annotations per image | >5 | Database query |
| Images meeting quality standards | >90% | Validation script |
| Broken image links | 0% | Automated checks |

---

## 9. Implementation Roadmap

### Phase 1: Critical Fixes (Week 1)
- [ ] Fix Visual Discrimination image rendering
- [ ] Implement image URL resolution API
- [ ] Add image loading states
- [ ] Add error handling for failed images
- [ ] Update type definitions for consistency

**Deliverable:** Visual Discrimination fully functional with real images

### Phase 2: Data Integration (Week 2)
- [ ] Connect annotations.imageId to image_sources table
- [ ] Implement image caching strategy
- [ ] Use bounding box data for Visual Identification hotspots
- [ ] Filter invisible annotations from exercises
- [ ] Add image quality validation to import

**Deliverable:** Dynamic exercise generation using annotation data

### Phase 3: Image Labeling (Week 3-4)
- [ ] Design ImageLabeling component UI
- [ ] Implement drag-and-drop interaction
- [ ] Create answer validation with tolerance
- [ ] Add visual feedback (connection lines)
- [ ] Integrate with exercise generator
- [ ] Add to exercise rotation

**Deliverable:** Complete Image Labeling exercise type

### Phase 4: Performance & UX (Week 5)
- [ ] Implement image preloading for next exercise
- [ ] Add service worker for offline caching
- [ ] Create thumbnail system for Visual Discrimination
- [ ] Add zoom/pan controls for detailed viewing
- [ ] Implement loading progress indicators

**Deliverable:** Smooth, performant exercise experience

### Phase 5: Advanced Features (Week 6+)
- [ ] Multi-image comparison exercises
- [ ] Adaptive difficulty based on image complexity
- [ ] Image gallery/study mode
- [ ] AI-powered image quality scoring
- [ ] User-generated content pipeline

**Deliverable:** Enhanced learning experience with advanced features

---

## 10. Conclusion

The AVES platform has a solid foundation for image-based exercises, with two of three types partially implemented. However, critical gaps prevent the system from functioning as intended:

**Strengths:**
- Well-defined type system for exercises
- Clean component architecture
- Good user interaction patterns
- Adaptive difficulty framework
- Backend image infrastructure exists

**Critical Blockers:**
1. Visual Discrimination renders emoji instead of images (CRITICAL)
2. No image URL resolution from annotation data (CRITICAL)
3. Image Labeling completely unimplemented (HIGH)
4. Hardcoded image limitations (HIGH)

**Highest Impact Improvements:**
1. Fix image rendering in Visual Discrimination (1 day effort, massive impact)
2. Connect annotation.imageId to actual images (2 days, unlocks dynamic content)
3. Implement Image Labeling exercise (1 week, adds major learning mode)
4. Use bounding box data for dynamic hotspots (3 days, scalability)

**Recommended Priority:**
Focus on Phase 1 and 2 immediately to make existing exercises fully functional, then implement Image Labeling in Phase 3 to complete the feature set. Performance optimizations in Phase 4 can follow once core functionality is solid.

The annotation system contains rich data (bounding boxes, difficulty levels, visibility flags) that is currently underutilized. Leveraging this data will enable dynamic, scalable exercise generation without hardcoded limitations.

---

## Appendix A: File Structure

```
aves/
‚îú‚îÄ‚îÄ frontend/src/
‚îÇ   ‚îú‚îÄ‚îÄ components/exercises/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ VisualIdentification.tsx       ‚úÖ Implemented (partial)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ VisualDiscrimination.tsx       ‚ö†Ô∏è  Implemented (broken images)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ImageLabeling.tsx              ‚ùå NOT IMPLEMENTED
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ContextualFill.tsx             ‚úÖ Implemented (no images)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ExerciseContainer.tsx          ‚úÖ Orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enhancedExerciseGenerator.ts   ‚úÖ Core logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ exerciseGenerator.ts           ‚úÖ Basic version
‚îÇ   ‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useExercise.ts                 ‚úÖ State management
‚îÇ   ‚îî‚îÄ‚îÄ types/
‚îÇ       ‚îú‚îÄ‚îÄ api.types.ts                   ‚úÖ API types
‚îÇ       ‚îî‚îÄ‚îÄ guards.ts                      ‚úÖ Type guards
‚îú‚îÄ‚îÄ shared/types/
‚îÇ   ‚îú‚îÄ‚îÄ exercise.types.ts                  ‚úÖ Exercise definitions
‚îÇ   ‚îú‚îÄ‚îÄ annotation.types.ts                ‚úÖ Annotation/Image types
‚îÇ   ‚îú‚îÄ‚îÄ enhanced-exercise.types.ts         ‚úÖ Enhanced types
‚îÇ   ‚îî‚îÄ‚îÄ image.types.ts                     ‚úÖ Image source types
‚îî‚îÄ‚îÄ backend/src/
    ‚îú‚îÄ‚îÄ routes/
    ‚îÇ   ‚îú‚îÄ‚îÄ images.ts                      ‚úÖ Image management
    ‚îÇ   ‚îî‚îÄ‚îÄ exercises.ts                   ‚úÖ Exercise tracking
    ‚îî‚îÄ‚îÄ services/
        ‚îî‚îÄ‚îÄ exerciseService.ts             ‚úÖ Exercise business logic
```

## Appendix B: Data Flow Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PostgreSQL  ‚îÇ
‚îÇ   Database   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îú‚îÄ species table
       ‚îú‚îÄ image_sources table ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îú‚îÄ annotations table           ‚îÇ
       ‚îî‚îÄ exercise_sessions table     ‚îÇ
                                      ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Backend API     ‚îÇ
‚îÇ  /api/images     ‚îÇ ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  /api/exercises  ‚îÇ    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
                        ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Frontend Services       ‚îÇ
‚îÇ  - exerciseGenerator     ‚îÇ
‚îÇ  - imagePreloader (TODO) ‚îÇ
‚îÇ  - imageCache (TODO)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Exercise Components     ‚îÇ
‚îÇ  - VisualIdentification  ‚îÇ
‚îÇ  - VisualDiscrimination  ‚îÇ
‚îÇ  - ImageLabeling (TODO)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  User Interface          ‚îÇ
‚îÇ  - Image display         ‚îÇ
‚îÇ  - Interaction           ‚îÇ
‚îÇ  - Feedback              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Current Broken Link:** image_sources table ‚Üí Frontend (no API endpoint for image retrieval by annotation.imageId)

---

**Document Version:** 1.0
**Last Updated:** October 2, 2025
**Next Review:** After Phase 1 implementation
